{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU5duosElDbLcevQ+pdsqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cinnaavox/favorita_sales_forecasting/blob/main/Week_2_3_modeling_and_evaluation_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U2620MiCvjNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 0 — Setup (ruhigstellen, Pfade, Imports)"
      ],
      "metadata": {
        "id": "0H3sBmUpvlqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --no-cache-dir \\\n",
        "  numpy==2.0.2 \\\n",
        "  pandas==2.2.2 \\\n",
        "  scikit-learn==1.6.1 \\\n",
        "  xgboost==2.1.1 \\\n",
        "  mlflow==3.1.1 \\\n",
        "  pyarrow==18.1.0"
      ],
      "metadata": {
        "id": "vVJF4m0uv-qG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings, logging, os, json, math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams[\"figure.dpi\"] = 120\n",
        "mpl.rcParams[\"axes.grid\"] = True\n",
        "mpl.rcParams[\"grid.linestyle\"] = \":\"\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import mlflow, mlflow.sklearn\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"VERSIONS ➜\",\n",
        "      \"numpy\", np.__version__,\n",
        "      \"| pandas\", pd.__version__,\n",
        "      \"| sklearn\", __import__(\"sklearn\").__version__,\n",
        "      \"| xgboost\", xgb.__version__,\n",
        "      \"| mlflow\", mlflow.__version__)\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "DATA_DIR   = f\"{DRIVE_ROOT}/time_series_course_guayas\"\n",
        "PARQUET    = f\"{DATA_DIR}/guayas_Q1_2014_ml_ready_favorita.parquet\"\n",
        "ART_DIR    = f\"{DATA_DIR}/artifacts_week2_3\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "print(\"Quell-Datei:\", PARQUET)\n",
        "print(\"Artefakt-Ordner:\", ART_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj2ZFIFmvmsQ",
        "outputId": "6a0dca0f-f0a2-4cb4-dd44-3281195a942b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VERSIONS ➜ numpy 2.0.2 | pandas 2.2.2 | sklearn 1.6.1 | xgboost 2.1.1 | mlflow 3.1.1\n",
            "Quell-Datei: /content/drive/MyDrive/time_series_course_guayas/guayas_Q1_2014_ml_ready_favorita.parquet\n",
            "Artefakt-Ordner: /content/drive/MyDrive/time_series_course_guayas/artifacts_week2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 1 — Laden & Sanity-Checks (Q1-Fenster, Spalten)"
      ],
      "metadata": {
        "id": "lWA3dRs2wWba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(PARQUET)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "need = {\"date\",\"store_nbr\",\"item_nbr\",\"unit_sales\"}\n",
        "miss = list(need - set(df.columns))\n",
        "if miss:\n",
        "    raise ValueError(f\"Fehlende Spalten im Parquet: {miss}\")\n",
        "\n",
        "# Q1 2014 (dein Export ist bereits Q1; wir klemmen trotzdem zu Sicherheit)\n",
        "df = df[(df[\"date\"] >= \"2014-01-01\") & (df[\"date\"] < \"2014-04-01\")].copy()\n",
        "\n",
        "# Zusatzfeatures aus Week 1 sind (typischerweise) vorhanden:\n",
        "extra_hint = [c for c in [\"lag_1\",\"lag_7\",\"lag_14\",\"lag_30\",\"roll_mean_7\",\"roll_std_7\",\"transactions\",\"dcoilwtico\"]\n",
        "              if c in df.columns]\n",
        "print(\"Zeitraum:\", df[\"date\"].min(), \"→\", df[\"date\"].max(), \"| rows:\", len(df))\n",
        "print(\"Zusatzfeatures gefunden:\", extra_hint)\n",
        "print(\"✅ Block 1 fertig.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "yQPOgyWswVyw",
        "outputId": "c3650ff4-2e66-4e25-ac82-eb1abee876c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/time_series_course_guayas/guayas_Q1_2014_ml_ready_favorita.parquet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-198250143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARQUET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"store_nbr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"item_nbr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unit_sales\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneed\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/time_series_course_guayas/guayas_Q1_2014_ml_ready_favorita.parquet'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 2 — Feature Engineering (minimal, stabil & schnell)"
      ],
      "metadata": {
        "id": "eWmb2gMBwbog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kalenderfeatures\n",
        "df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n",
        "df[\"month\"]       = df[\"date\"].dt.month\n",
        "df[\"is_weekend\"]  = (df[\"day_of_week\"]>=5).astype(int)\n",
        "\n",
        "# moderate Zusatz-Lags: 2,3,5,10,21\n",
        "by_keys = [\"store_nbr\",\"item_nbr\"]\n",
        "for L in [2,3,5,10,21]:\n",
        "    name = f\"lag_{L}\"\n",
        "    if name not in df.columns:\n",
        "        df[name] = df.groupby(by_keys)[\"unit_sales\"].shift(L)\n",
        "\n",
        "# Ratio-Features (robust)\n",
        "def safe_div(a,b):\n",
        "    return a / (b.replace(0,np.nan) + 1e-9)\n",
        "\n",
        "if \"roll_mean_7\" in df.columns and \"unit_sales\" in df.columns:\n",
        "    df[\"ratio_1_7\"]  = df[\"lag_1\"]  / (df[\"roll_mean_7\"] + 1)\n",
        "    df[\"ratio_7_30\"] = df[\"lag_7\"]  / (df[\"lag_30\"].replace(0,np.nan) + 1)\n",
        "\n",
        "# Promotions: falls als bool/str vorhanden → sauber zu int\n",
        "if \"onpromotion\" in df.columns:\n",
        "    df[\"onpromotion\"] = (\n",
        "        df[\"onpromotion\"]\n",
        "        .astype(str).str.lower()\n",
        "        .map({\"true\":1,\"false\":0,\"1\":1,\"0\":0})\n",
        "        .fillna(0).astype(int)\n",
        "    )\n",
        "\n",
        "# Drop Anfangs-NaNs nur für Trainingsfenster\n",
        "base_feats = [\"store_nbr\",\"item_nbr\",\"day_of_week\",\"month\",\"is_weekend\",\n",
        "              \"lag_1\",\"lag_7\",\"lag_14\",\"lag_30\",\"roll_mean_7\",\"transactions\",\"dcoilwtico\"]\n",
        "extra_feats = [c for c in [ \"roll_std_7\",\"lag_2\",\"lag_3\",\"lag_5\",\"lag_10\",\"lag_21\",\"ratio_1_7\",\"ratio_7_30\",\"onpromotion\" ]\n",
        "               if c in df.columns]\n",
        "FEATURES = base_feats + extra_feats\n",
        "TARGET   = \"unit_sales\"\n",
        "\n",
        "print(\"FEATURES:\", FEATURES)"
      ],
      "metadata": {
        "id": "uusNmHKUwcXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 3 — Zeitliche Splits (Jan–Feb Train / letzte 7 Tage Val / März Test)"
      ],
      "metadata": {
        "id": "CgGLxMvCwgOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Harte Grenzen (keine Shuffle/Leakage)\n",
        "cut_train_end = pd.Timestamp(\"2014-02-28\")           # Train bis Ende Februar\n",
        "val_days      = 7                                     # Mini-Validation = letzte 7 Feb-Tage\n",
        "val_start     = cut_train_end - pd.Timedelta(days=val_days-1)\n",
        "\n",
        "train_mask = (df[\"date\"] <= cut_train_end)\n",
        "val_mask   = (df[\"date\"] >= val_start) & (df[\"date\"] <= cut_train_end)\n",
        "# (Val ist Teil von Train -> ich nehme für \"Train pur\" alles <= cut ohne Val-Fenster)\n",
        "train_pure_mask = (df[\"date\"] < val_start)\n",
        "\n",
        "test_mask  = (df[\"date\"] >= \"2014-03-01\") & (df[\"date\"] < \"2014-04-01\")\n",
        "\n",
        "train_all = df.loc[train_mask].copy()\n",
        "train_pure= df.loc[train_pure_mask].copy()\n",
        "valid     = df.loc[val_mask].copy()\n",
        "test      = df.loc[test_mask].copy()\n",
        "\n",
        "# NaNs nur in den Modell-Features / Target droppen – jeweils pro Split\n",
        "def dropna_slice(d):\n",
        "    return d.dropna(subset=FEATURES+[TARGET]).copy()\n",
        "\n",
        "train_all = dropna_slice(train_all)\n",
        "train_pure= dropna_slice(train_pure)\n",
        "valid     = dropna_slice(valid)\n",
        "test      = dropna_slice(test)\n",
        "\n",
        "print(\n",
        "    \"Train(pure)/Val/Test rows:\", len(train_pure), \"/\", len(valid), \"/\", len(test),\n",
        "    \"| Test-Monate:\", test[\"date\"].dt.to_period(\"M\").value_counts().sort_index().to_dict()\n",
        ")"
      ],
      "metadata": {
        "id": "NfezFby6whHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 4 — Baseline XGBoost (Core-API + EarlyStopping) & Evaluation"
      ],
      "metadata": {
        "id": "eNqcJiIGwouI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DMatrix\n",
        "dtrain = xgb.DMatrix(train_pure[FEATURES], label=train_pure[TARGET])\n",
        "dvalid = xgb.DMatrix(valid[FEATURES],      label=valid[TARGET])\n",
        "dtest  = xgb.DMatrix(test[FEATURES])\n",
        "\n",
        "# Baseline-Params\n",
        "params_base = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eta\": 0.07,\n",
        "    \"max_depth\": 6,\n",
        "    \"subsample\": 0.9,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"reg_lambda\": 1.0,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "evals = [(dtrain,\"train\"), (dvalid,\"valid\")]\n",
        "booster_base = xgb.train(\n",
        "    params=params_base,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=2000,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=40,\n",
        "    verbose_eval=False,\n",
        ")\n",
        "\n",
        "# Prognose März + Metriken\n",
        "y_pred_base = booster_base.predict(dtest, iteration_range=(0, booster_base.best_iteration+1))\n",
        "y_true      = test[TARGET].values\n",
        "\n",
        "def metrics_all(y, yhat):\n",
        "    mae  = mean_absolute_error(y, yhat)\n",
        "    rmse = math.sqrt(mean_squared_error(y, yhat))\n",
        "    bias = float(np.mean(yhat - y))\n",
        "    mad  = float(np.mean(np.abs(yhat - y)))\n",
        "    rmad = float(mad / (np.mean(y) + 1e-9))\n",
        "    mape = float(np.mean(np.abs((y - yhat) / (np.maximum(np.abs(y), 1e-9)))) * 100)\n",
        "    smape= float(np.mean(2*np.abs(yhat - y)/(np.abs(yhat)+np.abs(y)+1e-9))*100)\n",
        "    return dict(MAE=mae, RMSE=rmse, Bias=bias, MAD=mad, rMAD=rmad, MAPE=mape, sMAPE=smape)\n",
        "\n",
        "m_base = metrics_all(y_true, y_pred_base)\n",
        "print(\"Baseline XGB:\", {k:round(v,4) for k,v in m_base.items()})"
      ],
      "metadata": {
        "id": "S1cBbRS0wpfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 5 — Plots (Tages-Summen, Heatmap, Feature-Importance) + Artefakte"
      ],
      "metadata": {
        "id": "KT0f-XLhwsnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Tages-Summen (März) – Actual vs Pred\n",
        "plot_df = test[[\"date\"]].copy()\n",
        "plot_df[\"y_true\"] = y_true\n",
        "plot_df[\"y_pred\"] = y_pred_base\n",
        "daily = plot_df.groupby(\"date\", as_index=False).sum()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(daily[\"date\"], daily[\"y_true\"], label=\"Actual (sum)\", linewidth=2)\n",
        "plt.plot(daily[\"date\"], daily[\"y_pred\"], label=\"Pred (sum)\", linewidth=2, alpha=0.9)\n",
        "plt.title(\"März 2014 — Tages-Summe: Actual vs Predicted\")\n",
        "plt.legend(); plt.xlabel(\"Datum\"); plt.ylabel(\"Units\")\n",
        "fig1_path = f\"{ART_DIR}/daily_sum_baseline.png\"\n",
        "plt.tight_layout(); plt.savefig(fig1_path); plt.show()\n",
        "\n",
        "# 2) Fehler-Heatmap (Store × Wochentag, MAE)\n",
        "err_df = test[[\"date\",\"store_nbr\"]].copy()\n",
        "err_df[\"ae\"]  = np.abs(y_true - y_pred_base)\n",
        "err_df[\"dow\"] = err_df[\"date\"].dt.dayofweek\n",
        "pivot = err_df.pivot_table(index=\"store_nbr\", columns=\"dow\", values=\"ae\", aggfunc=\"mean\").fillna(0)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.imshow(pivot.values, aspect=\"auto\")\n",
        "plt.xticks(range(7), [\"Mo\",\"Di\",\"Mi\",\"Do\",\"Fr\",\"Sa\",\"So\"])\n",
        "plt.yticks(range(len(pivot.index)), pivot.index)\n",
        "plt.title(\"Ø Absoluter Fehler nach Store × Wochentag (März)\")\n",
        "cbar = plt.colorbar(); cbar.set_label(\"MAE\")\n",
        "fig2_path = f\"{ART_DIR}/heatmap_store_dow_baseline.png\"\n",
        "plt.tight_layout(); plt.savefig(fig2_path); plt.show()\n",
        "\n",
        "# 3) Feature Importance (Gain) – nach Namen\n",
        "# Mapping\n",
        "fscore = booster_base.get_score(importance_type=\"gain\")\n",
        "imp_df = pd.DataFrame(\n",
        "    {\"Feature\": list(fscore.keys()), \"Gain\": list(fscore.values())}\n",
        ").sort_values(\"Gain\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(imp_df[\"Feature\"], imp_df[\"Gain\"])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"XGBoost Feature Importance (Gain) — Baseline\")\n",
        "plt.xlabel(\"Gain\")\n",
        "fig3_path = f\"{ART_DIR}/feature_importance_baseline.png\"\n",
        "plt.tight_layout(); plt.savefig(fig3_path); plt.show()\n",
        "\n",
        "print(\"Artefakte gespeichert:\", [fig1_path, fig2_path, fig3_path])"
      ],
      "metadata": {
        "id": "yO1AgaVVwt3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 6 — MLflow: Baseline-Run loggen"
      ],
      "metadata": {
        "id": "zn49af6hwyWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"Retail Forecast Models (Guayas Q1 2014)\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"Baseline_XGB\"):\n",
        "    mlflow.set_tag(\"model\", \"XGBoost\")\n",
        "    mlflow.log_params(params_base)\n",
        "    for k,v in m_base.items():\n",
        "        mlflow.log_metric(k, float(v))\n",
        "    # Plots als Artefakte\n",
        "    mlflow.log_artifact(fig1_path)\n",
        "    mlflow.log_artifact(fig2_path)\n",
        "    mlflow.log_artifact(fig3_path)\n",
        "    # Booster als Modell (Sklearn-Wrapper umgehen → pkl via Booster.save_model)\n",
        "    model_path = f\"{ART_DIR}/xgb_baseline.json\"\n",
        "    booster_base.save_model(model_path)\n",
        "    mlflow.log_artifact(model_path)"
      ],
      "metadata": {
        "id": "VoAM6pLNwzN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 7 — Tuning (schlankes Random-Search, Zeitreihen-sauber)"
      ],
      "metadata": {
        "id": "I6GbsXDHw2Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools, random\n",
        "random.seed(42)\n",
        "\n",
        "search_space = {\n",
        "    \"eta\":              [0.03, 0.05, 0.07, 0.1],\n",
        "    \"max_depth\":        [4,5,6,7],\n",
        "    \"subsample\":        [0.7, 0.85, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.85, 1.0],\n",
        "    \"reg_lambda\":       [0.5, 1.0, 2.0],\n",
        "    \"min_child_weight\": [1, 3, 5],\n",
        "}\n",
        "\n",
        "def sample_configs(space, n=25):\n",
        "    # zufällige Kombinationen\n",
        "    keys = list(space.keys())\n",
        "    choices = [space[k] for k in keys]\n",
        "    all_combos = list(itertools.product(*choices))\n",
        "    random.shuffle(all_combos)\n",
        "    for combo in all_combos[:n]:\n",
        "        yield dict(zip(keys, combo))\n",
        "\n",
        "best_rmse = float(\"inf\")\n",
        "best_cfg  = None\n",
        "best_booster = None\n",
        "\n",
        "for i, cfg in enumerate(sample_configs(search_space, n=25), 1):\n",
        "    params = {\n",
        "        **params_base,              # Basis bleibt\n",
        "        \"eta\": cfg[\"eta\"],\n",
        "        \"max_depth\": cfg[\"max_depth\"],\n",
        "        \"subsample\": cfg[\"subsample\"],\n",
        "        \"colsample_bytree\": cfg[\"colsample_bytree\"],\n",
        "        \"reg_lambda\": cfg[\"reg_lambda\"],\n",
        "        \"min_child_weight\": cfg[\"min_child_weight\"],\n",
        "    }\n",
        "    bst = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=2000,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False,\n",
        "    )\n",
        "    # Val-RMSE ist eval_metric\n",
        "    rmse_val = float(bst.best_score)\n",
        "    if rmse_val < best_rmse:\n",
        "        best_rmse = rmse_val\n",
        "        best_cfg  = params\n",
        "        best_booster = bst\n",
        "\n",
        "print(\"Bestes Val-RMSE:\", round(best_rmse,4))\n",
        "print(\"Beste Params:\", {k:best_cfg[k] for k in [\"eta\",\"max_depth\",\"subsample\",\"colsample_bytree\",\"reg_lambda\",\"min_child_weight\"]})"
      ],
      "metadata": {
        "id": "1qQttVGtw3gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 8 — Best-Model testen (März), visualisieren & MLflow-Run #2"
      ],
      "metadata": {
        "id": "z3A3mJd_w7Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test auf März\n",
        "y_pred_best = best_booster.predict(dtest, iteration_range=(0, best_booster.best_iteration+1))\n",
        "m_best = metrics_all(y_true, y_pred_best)\n",
        "print(\"Best Tuned XGB:\", {k:round(v,4) for k,v in m_best.items()})\n",
        "\n",
        "# Plot (Tages-Summen) neu mit Best\n",
        "plot_df2 = test[[\"date\"]].copy()\n",
        "plot_df2[\"y_true\"] = y_true\n",
        "plot_df2[\"y_pred\"] = y_pred_best\n",
        "daily2 = plot_df2.groupby(\"date\", as_index=False).sum()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(daily2[\"date\"], daily2[\"y_true\"], label=\"Actual (sum)\", linewidth=2)\n",
        "plt.plot(daily2[\"date\"], daily2[\"y_pred\"], label=\"Pred (sum) — Tuned\", linewidth=2, alpha=0.9)\n",
        "plt.title(\"März 2014 — Tages-Summe: Actual vs Predicted (Tuned)\")\n",
        "plt.legend(); plt.xlabel(\"Datum\"); plt.ylabel(\"Units\")\n",
        "fig_best_path = f\"{ART_DIR}/daily_sum_tuned.png\"\n",
        "plt.tight_layout(); plt.savefig(fig_best_path); plt.show()\n",
        "\n",
        "# MLflow: Run #2 (nur Gewinner-Konfig)\n",
        "with mlflow.start_run(run_name=\"Best_XGB_Tuned\"):\n",
        "    mlflow.set_tag(\"model\", \"XGBoost\")\n",
        "    tune_params = {k: best_cfg[k] for k in [\"eta\",\"max_depth\",\"subsample\",\"colsample_bytree\",\"reg_lambda\",\"min_child_weight\"]}\n",
        "    mlflow.log_params(tune_params)\n",
        "    for k,v in m_best.items():\n",
        "        mlflow.log_metric(k, float(v))\n",
        "    # Artefakt\n",
        "    mlflow.log_artifact(fig_best_path)\n",
        "    # Modell\n",
        "    best_model_path = f\"{ART_DIR}/xgb_best.json\"\n",
        "    best_booster.save_model(best_model_path)\n",
        "    mlflow.log_artifact(best_model_path)"
      ],
      "metadata": {
        "id": "MzgCWPf4w8Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 9 — Artefakte für Week 4 speichern (Feature-Liste, Modelle)"
      ],
      "metadata": {
        "id": "OBngdasNxAQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature-Liste & Meta\n",
        "prep_meta = {\n",
        "    \"features\": FEATURES,\n",
        "    \"target\": TARGET,\n",
        "    \"train_window\": \"2014-01-01 … 2014-02-28 (Val = letzte 7 Tage Feb)\",\n",
        "    \"test_window\":  \"2014-03 (März)\",\n",
        "    \"dataset\": os.path.basename(PARQUET),\n",
        "}\n",
        "with open(f\"{ART_DIR}/preprocessing_meta.json\",\"w\") as f:\n",
        "    json.dump(prep_meta, f, indent=2)\n",
        "\n",
        "# Modelle ablegen (für Streamlit später)\n",
        "save_dir_models = f\"{DRIVE_ROOT}/models\"\n",
        "os.makedirs(save_dir_models, exist_ok=True)\n",
        "Path(f\"{save_dir_models}/xgb_baseline.json\").write_text(Path(f\"{ART_DIR}/xgb_baseline.json\").read_text())\n",
        "Path(f\"{save_dir_models}/xgb_best.json\").write_text(Path(f\"{ART_DIR}/xgb_best.json\").read_text())\n",
        "\n",
        "print(\"Gespeichert:\")\n",
        "print(\" -\", f\"{ART_DIR}/preprocessing_meta.json\")\n",
        "print(\" -\", f\"{save_dir_models}/xgb_baseline.json\")\n",
        "print(\" -\", f\"{save_dir_models}/xgb_best.json\")"
      ],
      "metadata": {
        "id": "elnj6WemxBGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 10 — Mini-LSTM zum Vergleichen"
      ],
      "metadata": {
        "id": "Do7lhMWpxELy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "import os, warnings, numpy as np, matplotlib.pyplot as plt, math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Daten zusammenführen (Train + Test) wie im XGB-Teil\n",
        "xgb_df = pd.concat([train_all, test], axis=0, ignore_index=True)\n",
        "xgb_df = xgb_df.dropna(subset=FEATURES + [TARGET]).copy()\n",
        "\n",
        "X_all = xgb_df[FEATURES].values\n",
        "y_all = xgb_df[TARGET].values.reshape(-1, 1)\n",
        "\n",
        "# Skalierung\n",
        "sc_x = MinMaxScaler()\n",
        "sc_y = MinMaxScaler()\n",
        "X_all_s = sc_x.fit_transform(X_all).astype(np.float32)\n",
        "y_all_s = sc_y.fit_transform(y_all).astype(np.float32)\n",
        "\n",
        "# Sequenzen bauen (rolling window)\n",
        "def make_seq(X, y, win=30):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(win, len(X)):\n",
        "        Xs.append(X[i - win:i])\n",
        "        ys.append(y[i])\n",
        "    return np.asarray(Xs, dtype=np.float32), np.asarray(ys, dtype=np.float32)\n",
        "\n",
        "WIN = 30\n",
        "Xs, ys = make_seq(X_all_s, y_all_s, win=WIN)\n",
        "\n",
        "if Xs.shape[0] < 150:\n",
        "    WIN = 14\n",
        "    Xs, ys = make_seq(X_all_s, y_all_s, win=WIN)\n",
        "if Xs.shape[0] < 80:\n",
        "    WIN = 7\n",
        "    Xs, ys = make_seq(X_all_s, y_all_s, win=WIN)\n",
        "\n",
        "assert Xs.shape[0] > 50, f\"Zu wenig Sequenzen ({Xs.shape[0]}). WIN={WIN} ist Minimum – check vorherige Blöcke.\"\n",
        "\n",
        "# Zeitkonsistenter Split analog XGB\n",
        "n_train = len(train_all.dropna(subset=FEATURES + [TARGET]))\n",
        "n_test  = len(test.dropna(subset=FEATURES + [TARGET]))\n",
        "n_train_seq = max(n_train - WIN, 0)\n",
        "assert n_train_seq > 20, \"Train-Teil für LSTM zu kurz – prüfe Splits / WIN.\"\n",
        "\n",
        "Xtr, Xte = Xs[:n_train_seq], Xs[n_train_seq:]\n",
        "ytr, yte = ys[:n_train_seq], ys[n_train_seq:]\n",
        "\n",
        "# Torch-Tensors\n",
        "Xtr_t = torch.from_numpy(Xtr)  # (N, WIN, F)\n",
        "ytr_t = torch.from_numpy(ytr)  # (N, 1)\n",
        "Xte_t = torch.from_numpy(Xte)\n",
        "yte_t = torch.from_numpy(yte)\n",
        "\n",
        "# DataLoader (+ kleine Val-Abspaltung aus Train, zeitlich KONSERVATIV → letztes Stück als Val)\n",
        "train_ratio = 0.9\n",
        "n_total = len(Xtr_t)\n",
        "n_train_sub = int(n_total * train_ratio)\n",
        "n_val_sub = n_total - n_train_sub\n",
        "train_ds, val_ds = random_split(\n",
        "    TensorDataset(Xtr_t, ytr_t),\n",
        "    [n_train_sub, n_val_sub],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Mini-LSTM Architektur\n",
        "class MiniLSTM(nn.Module):\n",
        "    def __init__(self, in_feats, hidden1=96, hidden2=32, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size=in_feats, hidden_size=hidden1, batch_first=True)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden1, hidden_size=hidden2, batch_first=True)\n",
        "        self.fc1   = nn.Linear(hidden2, 16)\n",
        "        self.act   = nn.ReLU()\n",
        "        self.out   = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, WIN, F)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.drop(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = x[:, -1, :]         # letztes Zeitschritt-Embedding\n",
        "        x = self.act(self.fc1(x))\n",
        "        return self.out(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MiniLSTM(in_feats=Xtr.shape[2]).to(device)\n",
        "\n",
        "# Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 25\n",
        "patience = 3\n",
        "best_val = float(\"inf\")\n",
        "wait = 0\n",
        "best_state = None\n",
        "\n",
        "def run_epoch(dl, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    loss_sum, n = 0.0, 0\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for xb, yb in dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            loss_sum += loss.item() * xb.size(0)\n",
        "            n += xb.size(0)\n",
        "    return loss_sum / max(n, 1)\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr_loss = run_epoch(train_loader, train=True)\n",
        "    vl_loss = run_epoch(val_loader,   train=False) if len(val_ds) > 0 else tr_loss\n",
        "    # Early Stopping\n",
        "    if vl_loss < best_val - 1e-6:\n",
        "        best_val = vl_loss\n",
        "        wait = 0\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            break\n",
        "\n",
        "# Bestes State laden\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# Inferenz (Test)\n",
        "model.eval()\n",
        "preds_s, trues_s = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        pb = model(xb).cpu().numpy()\n",
        "        preds_s.append(pb)\n",
        "        trues_s.append(yb.numpy())\n",
        "\n",
        "yp_s = np.vstack(preds_s)             # scaled\n",
        "yt_s = np.vstack(trues_s)\n",
        "\n",
        "# Rückskalieren\n",
        "yp = sc_y.inverse_transform(yp_s).ravel()\n",
        "y_true_lstm = xgb_df.iloc[n_train:][TARGET].values\n",
        "y_true_lstm = y_true_lstm[-len(yp):]\n",
        "\n",
        "# Metriken & Plot\n",
        "m_lstm = metrics_all(y_true_lstm, yp)\n",
        "print(\"LSTM (optional, PyTorch):\", {k: round(float(v), 4) for k, v in m_lstm.items()})\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(y_true_lstm[:100], label=\"Actual\", linewidth=2)\n",
        "plt.plot(yp[:100],          label=\"LSTM\",   linestyle=\"--\", linewidth=2)\n",
        "plt.title(\"LSTM Forecast – first 100 samples (optional, PyTorch)\")\n",
        "plt.legend(); plt.tight_layout(); plt.grid(True, ls=\":\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y8X7WcLwxE-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 11 — Leak-/Split-Checks & Grund-Metriken"
      ],
      "metadata": {
        "id": "dSEU0YTo3tTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prüfen, ob Features versehentlich Zukunft sehen & ob Splits passen\n",
        "assert set(FEATURES).issubset(set(train_all.columns)), \"FEATURES fehlen in train_all.\"\n",
        "assert TARGET in train_all.columns, \"TARGET fehlt.\"\n",
        "\n",
        "def check_leakage(df, time_col=\"date\", group_cols=[\"store_nbr\",\"item_nbr\"]):\n",
        "    # einfache Heuristik: Lags dürfen nicht NaN-frei direkt am Fold-Schnitt starten\n",
        "    suspects = []\n",
        "    for c in FEATURES:\n",
        "        if \"lag\" in c or \"avg\" in c:\n",
        "            frac_na = df[c].isna().mean()\n",
        "            if frac_na == 0.0:\n",
        "                suspects.append(c)\n",
        "    return suspects\n",
        "\n",
        "xgb_df = pd.concat([train_all, test], axis=0, ignore_index=True)\n",
        "leaks = check_leakage(xgb_df)\n",
        "print(\"🔎 Potenzielle Leak-Features (prüfen!):\", leaks or \"keine gefunden\")\n",
        "\n",
        "# Baseline-Metriken deines existierenden XGB (falls 'yp_xgb_test' existiert) und LSTM (Block 10)\n",
        "if 'yp_xgb_test' in globals() and 'y_true_xgb' in globals():\n",
        "    print(\"XGB Test:\", {k: round(v,4) for k,v in metrics_all(y_true_xgb, yp_xgb_test).items()})\n",
        "# LSTM alias (aus Block 10 heißt das Modell 'model')\n",
        "try:\n",
        "    model_lstm_alias = model  # MiniLSTM aus Block 10\n",
        "except NameError:\n",
        "    model_lstm_alias = None"
      ],
      "metadata": {
        "id": "N6smik5H3uIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 12 — Rolling-Origin Backtesting (XGBoost, Early Stopping)"
      ],
      "metadata": {
        "id": "5E2p2qyi3xJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "# Daten wie im XGB-Teil aufbereiten\n",
        "df_bt = xgb_df.dropna(subset=FEATURES+[TARGET]).copy()\n",
        "X = df_bt[FEATURES].values\n",
        "y = df_bt[TARGET].values\n",
        "\n",
        "# 5 Folds, expandierend (rolling-origin)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "fold_metrics = []\n",
        "fold_preds = np.full_like(y, fill_value=np.nan, dtype=float)\n",
        "\n",
        "for f,(tr, te) in enumerate(tscv.split(X, y), 1):\n",
        "    dtr = xgb.DMatrix(X[tr], label=y[tr])\n",
        "    dte = xgb.DMatrix(X[te], label=y[te])\n",
        "\n",
        "    params = dict(\n",
        "        objective=\"reg:squarederror\",\n",
        "        eval_metric=\"rmse\",\n",
        "        eta=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        min_child_weight=3,\n",
        "        tree_method=\"hist\"\n",
        "    )\n",
        "    evallist = [(dtr,\"train\"),(dte,\"valid\")]\n",
        "    booster = xgb.train(\n",
        "        params, dtr, num_boost_round=2000, evals=evallist,\n",
        "        early_stopping_rounds=50, verbose_eval=False\n",
        "    )\n",
        "    yp = booster.predict(dte)\n",
        "    fold_preds[te] = yp\n",
        "    m = metrics_all(y[te], yp); fold_metrics.append(m)\n",
        "    print(f\"Fold {f}: \", {k: round(v,4) for k,v in m.items()})\n",
        "\n",
        "print(\"⛳ Backtest-Ø:\", {k: round(np.nanmean([m[k] for m in fold_metrics]),4) for k in fold_metrics[0].keys()})\n",
        "\n",
        "# Vis: letztes Fold\n",
        "plt.figure(figsize=(12,4))\n",
        "last_te = list(tscv.split(X,y))[-1][1]\n",
        "plt.plot(y[last_te][:200], label=\"Actual\")\n",
        "plt.plot(fold_preds[last_te][:200], \"--\", label=\"XGB BT pred\")\n",
        "plt.legend(); plt.title(\"XGB Rolling-Origin: letztes Fold (erst 200)\"); plt.grid(True, ls=\":\"); plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "6XptSMf73x7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 13 — Schnelles XGB-Tuning mit Optuna (robust)"
      ],
      "metadata": {
        "id": "xNGe-_mA31fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install optuna\n",
        "import optuna, numpy as np\n",
        "\n",
        "dall = xgb.DMatrix(X, label=y)\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\":\"reg:squarederror\",\n",
        "        \"eval_metric\":\"rmse\",\n",
        "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"tree_method\":\"hist\"\n",
        "    }\n",
        "    # val = letztes 10% als EarlyStopping-Val\n",
        "    n = len(y); cut = int(n*0.9)\n",
        "    dtr, dval = xgb.DMatrix(X[:cut], label=y[:cut]), xgb.DMatrix(X[cut:], label=y[cut:])\n",
        "    booster = xgb.train(params, dtr, 2000, [(dtr,\"train\"),(dval,\"valid\")], early_stopping_rounds=50, verbose_eval=False)\n",
        "    yp = booster.predict(dval)\n",
        "    return float(np.sqrt(((yp - y[cut:])**2).mean()))\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
        "best_params = study.best_params\n",
        "print(\"🏆 Optuna-Best:\", best_params)"
      ],
      "metadata": {
        "id": "LxjpJChS32Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 14 — SHAP für XGB (globale + lokale Erklärungen)"
      ],
      "metadata": {
        "id": "V27KTOw23_oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) finales XGB\n",
        "params_final = dict(\n",
        "    objective=\"reg:squarederror\",\n",
        "    eval_metric=\"rmse\",\n",
        "    tree_method=\"hist\",\n",
        ")\n",
        "if 'best_params' in globals():\n",
        "    params_final.update(best_params)\n",
        "\n",
        "dtrain = xgb.DMatrix(X, label=y)\n",
        "num_round = 400\n",
        "if 'study' in globals():\n",
        "\n",
        "    try:\n",
        "        num_round = max(50, int(study.best_trial.user_attrs.get(\"best_iteration\", 400)))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "booster_final = xgb.train(params_final, dtrain, num_boost_round=num_round)\n",
        "\n",
        "# 2) SHAP-Contributions direkt aus XGBoost holen\n",
        "#    Achtung: letzte Spalte ist der Bias-Term -> rauswerfen\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.random.choice(len(X), size=min(5000, len(X)), replace=False)\n",
        "dsub = xgb.DMatrix(X[idx])\n",
        "contribs = booster_final.predict(dsub, pred_contribs=True)   # shape: [n, F+1]\n",
        "shap_vals = contribs[:, :-1]                                 # drop bias column\n",
        "feature_names = np.array(FEATURES)\n",
        "\n",
        "# 3) Globale Wichtigkeit: mittlere |SHAP|-Beträge\n",
        "mean_abs = np.mean(np.abs(shap_vals), axis=0)\n",
        "order = np.argsort(mean_abs)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names[order], mean_abs[order])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"XGBoost SHAP (abs. mean) – ohne shap-Paket\")\n",
        "plt.xlabel(\"Durchschnitt |Beitrag|\")\n",
        "plt.grid(True, linestyle=\":\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4)einfache \"Beeswarm\"-ähnliche Streuung für Top-Features\n",
        "top_k = min(12, len(feature_names))\n",
        "top_feats = order[:top_k]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for rank, fidx in enumerate(top_feats):\n",
        "    y_jitter = rank + (np.random.rand(len(idx)) - 0.5) * 0.6\n",
        "    plt.scatter(shap_vals[:, fidx], y_jitter, s=6, alpha=0.35)\n",
        "plt.yticks(range(top_k), feature_names[top_feats])\n",
        "plt.title(\"XGBoost SHAP Scatter (Top-Features)\")\n",
        "plt.xlabel(\"SHAP-Beitrag (positiv ↑ treibt Prognose hoch)\")\n",
        "plt.grid(True, linestyle=\":\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5)lokale Erklärung für eine einzelne Zeile\n",
        "i_local = int(idx[0])  # nimm eine Beispieldatenzeile\n",
        "x_local = X[i_local:i_local+1]\n",
        "d_local = xgb.DMatrix(x_local)\n",
        "local_contrib = booster_final.predict(d_local, pred_contribs=True)[0]\n",
        "local_shap = local_contrib[:-1]\n",
        "bias = local_contrib[-1]\n",
        "\n",
        "top_local = np.argsort(np.abs(local_shap))[::-1][:10]\n",
        "print(\"🔎 Lokale Erklärung für Sample-Index\", i_local)\n",
        "for j in top_local:\n",
        "    print(f\"  {feature_names[j]:<25}  SHAP={local_shap[j]: .4f}\")\n",
        "print(f\"  {'(Bias)':<25}  {bias: .4f}\")"
      ],
      "metadata": {
        "id": "WiqEYhr34AXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 15 — Residual-Diagnostik (by Store/Item/DOW) + Ausreißer"
      ],
      "metadata": {
        "id": "-wyXMF124wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Residuals vorbereiten (Backtest-Vorhersagen vs. Ist)\n",
        "bt_df = df_bt.copy()\n",
        "bt_df[\"pred_bt\"] = np.asarray(fold_preds)\n",
        "bt_df[\"resid\"] = bt_df[TARGET] - bt_df[\"pred_bt\"]\n",
        "\n",
        "# kleine Schutzklausel, falls Längen nicht exakt matchen\n",
        "if len(bt_df) != len(fold_preds):\n",
        "    print(\"⚠️ Warnung: Länge bt_df != fold_preds – prüfe deine Backtest-Zusammenführung.\")\n",
        "\n",
        "# 1) eigene Aggregationsfunktionen\n",
        "def mean_abs(x):\n",
        "    return np.mean(np.abs(x))\n",
        "\n",
        "def median_abs(x):\n",
        "    return np.median(np.abs(x))\n",
        "\n",
        "def agg_and_show(key):\n",
        "    g = (\n",
        "        bt_df.dropna(subset=[\"resid\"])\n",
        "             .groupby(key)[\"resid\"]\n",
        "             .agg(\n",
        "                 mean=\"mean\",\n",
        "                 median=\"median\",\n",
        "                 mean_abs=mean_abs,\n",
        "                 med_abs=median_abs\n",
        "             )\n",
        "             .sort_values(\"med_abs\", ascending=False)\n",
        "    )\n",
        "    print(f\"🔧 Fehler nach {key} (Top 10 nach medianer |Abweichung|):\")\n",
        "    try:\n",
        "        display(g.head(10))\n",
        "    except NameError:\n",
        "        print(g.head(10))\n",
        "\n",
        "# 2) Gruppendiagnostik\n",
        "keys = [\"store_nbr\",\"item_nbr\"]\n",
        "if \"day_of_week\" in bt_df.columns:\n",
        "    keys.append(\"day_of_week\")\n",
        "\n",
        "for k in keys:\n",
        "    agg_and_show(k)\n",
        "\n",
        "# 3) Robuste Ausreißer-Erkennung via (unskaliertem) MAD um die Median-Residuen\n",
        "resid_clean = bt_df[\"resid\"].dropna().values\n",
        "med = np.median(resid_clean)\n",
        "mad = np.median(np.abs(resid_clean - med))  # \"median absolute deviation\"\n",
        "\n",
        "# Schwelle: 6 * MAD (robust). Falls MAD==0, fallback auf 6 * mean_abs\n",
        "thr = 6 * mad if mad > 0 else 6 * np.mean(np.abs(resid_clean))\n",
        "\n",
        "out_mask = bt_df[\"resid\"].abs() > thr\n",
        "out_idx = bt_df.index[out_mask]\n",
        "print(f\"⚠️ starke Ausreißer: {out_mask.sum()} Zeilen (Schwelle = {thr:.4f})\")\n",
        "\n",
        "# Ein paar Beispiele zeigen\n",
        "try:\n",
        "    display(bt_df.loc[out_idx, [\"date\",\"store_nbr\",\"item_nbr\",TARGET,\"pred_bt\",\"resid\"]].head(10))\n",
        "except NameError:\n",
        "    print(bt_df.loc[out_idx, [\"date\",\"store_nbr\",\"item_nbr\",TARGET,\"pred_bt\",\"resid\"]].head(10))\n",
        "\n",
        "# Histogramm der Residuen\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.hist(bt_df[\"resid\"].dropna(), bins=80)\n",
        "plt.axvline(med + thr, ls=\"--\")\n",
        "plt.axvline(med - thr, ls=\"--\")\n",
        "plt.title(\"Residualverteilung mit Ausreißer-Schwelle\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "joaEj38b4xXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 16 — Conformal Prediction Intervals (modellagnostisch)"
      ],
      "metadata": {
        "id": "jV84VLpb5wXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kalibriere auf letztem 10% von TRAIN (nicht TEST!), wende auf TEST an\n",
        "n_all = len(df_bt)\n",
        "cut = int(n_all*0.9)\n",
        "resid_cal = (bt_df.iloc[:cut][TARGET] - bt_df.iloc[:cut][\"pred_bt\"]).abs().dropna().values\n",
        "q = np.quantile(resid_cal, 0.9)  # 90%-Intervall; passe an (0.8/0.95 …)\n",
        "print(f\"📏 Conformal Halbbreite (90%): {q:.3f}\")\n",
        "\n",
        "bt_df[\"pi_lo\"] = bt_df[\"pred_bt\"] - q\n",
        "bt_df[\"pi_hi\"] = bt_df[\"pred_bt\"] + q\n",
        "\n",
        "# Coverage auf Holdout\n",
        "mask_hold = ~bt_df[\"pred_bt\"].iloc[cut:].isna()\n",
        "y_hold = bt_df[TARGET].iloc[cut:][mask_hold].values\n",
        "lo = bt_df[\"pi_lo\"].iloc[cut:][mask_hold].values\n",
        "hi = bt_df[\"pi_hi\"].iloc[cut:][mask_hold].values\n",
        "coverage = np.mean((y_hold >= lo) & (y_hold <= hi))\n",
        "print(f\"✅ Empirische Coverage ~{coverage*100:.1f}% (Ziel ~90%)\")"
      ],
      "metadata": {
        "id": "gyj-huiE5xJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 17 — Simple Ensemble (XGB ⊕ LSTM, Gewicht auto-getuned)"
      ],
      "metadata": {
        "id": "8KvJJbe75zzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'yp' in globals():\n",
        "    l = len(yp)\n",
        "    yp_lstm_full = np.full(len(df_bt), np.nan, dtype=float)\n",
        "    yp_lstm_full[-l:] = yp\n",
        "else:\n",
        "    raise RuntimeError(\"Keine LSTM-Preds (Variable 'yp') gefunden – bitte Block 10 ausführen.\")\n",
        "\n",
        "# Valid-Fenster = letztes 10%\n",
        "val_slice = slice(cut, len(df_bt))\n",
        "xgb_val = bt_df[\"pred_bt\"].values[val_slice]\n",
        "lstm_val = yp_lstm_full[val_slice]\n",
        "y_val   = df_bt[TARGET].values[val_slice]\n",
        "mask = ~np.isnan(xgb_val) & ~np.isnan(lstm_val)\n",
        "\n",
        "alphas = np.linspace(0,1,41)  # 0.0,0.025,…,1.0\n",
        "best_alpha, best_rmse = None, 1e9\n",
        "for a in alphas:\n",
        "    ens = a*lstm_val[mask] + (1-a)*xgb_val[mask]\n",
        "    rmse = np.sqrt(((ens - y_val[mask])**2).mean())\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse, best_alpha = rmse, a\n",
        "\n",
        "print(f\"🤝 Bestes Ensemblé-Gewicht: alpha(LSTM)={best_alpha:.3f}  → RMSE={best_rmse:.4f}\")\n",
        "\n",
        "# Finale Ensemblé-Reihe (wo beides vorhanden ist)\n",
        "ensemble_full = np.where(np.isnan(yp_lstm_full), bt_df[\"pred_bt\"].values, best_alpha*yp_lstm_full + (1-best_alpha)*bt_df[\"pred_bt\"].values)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(y_val[:200], label=\"Actual\")\n",
        "plt.plot(xgb_val[:200], \"--\", label=\"XGB\")\n",
        "plt.plot(lstm_val[:200], \"--\", label=\"LSTM\")\n",
        "plt.plot((best_alpha*lstm_val + (1-best_alpha)*xgb_val)[:200], label=\"Ensemble\", linewidth=2)\n",
        "plt.title(\"Ensemble (erst 200 im Val-Fenster)\"); plt.legend(); plt.grid(True, ls=\":\"); plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "IEsHk9ss50iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 18 — Artefakte speichern (für Week-4 Streamlit)"
      ],
      "metadata": {
        "id": "hgk5o8YX52kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, joblib, numpy as np, torch\n",
        "\n",
        "ART_DIR = ART_DIR if 'ART_DIR' in globals() else \"./artifacts_week2_3\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "# 1) XGB: Booster + Featureliste\n",
        "xgb_model_path = os.path.join(ART_DIR, \"xgb_booster.json\")\n",
        "booster_final.save_model(xgb_model_path)\n",
        "\n",
        "with open(os.path.join(ART_DIR, \"features.json\"), \"w\") as f:\n",
        "    json.dump({\"FEATURES\": FEATURES, \"TARGET\": TARGET}, f, indent=2)\n",
        "\n",
        "# 2) LSTM: TorchScript + Scaler für y/x\n",
        "try:\n",
        "    model_lstm_alias.eval()\n",
        "    example = torch.from_numpy(np.zeros((1, 30 if 'WIN' not in globals() else WIN, len(FEATURES)), dtype=np.float32))\n",
        "    traced = torch.jit.trace(model_lstm_alias, example)\n",
        "    torchscript_path = os.path.join(ART_DIR, \"lstm_torchscript.pt\")\n",
        "    traced.save(torchscript_path)\n",
        "    print(\"💾 LSTM TorchScript:\", torchscript_path)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Konnte LSTM nicht speichern (TorchScript). Grund:\", e)\n",
        "\n",
        "# Scaler (aus Block 10)\n",
        "try:\n",
        "    joblib.dump(sc_x, os.path.join(ART_DIR, \"scaler_X.pkl\"))\n",
        "    joblib.dump(sc_y, os.path.join(ART_DIR, \"scaler_y.pkl\"))\n",
        "    print(\"💾 Scaler gespeichert.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 3) Ensemble-Config\n",
        "with open(os.path.join(ART_DIR, \"ensemble.json\"), \"w\") as f:\n",
        "    json.dump({\"alpha_lstm\": float(best_alpha)}, f, indent=2)\n",
        "\n",
        "# 4) Helper für Streamlit (einfacher Predictor)\n",
        "predictor_py = f\"\"\"\n",
        "import json, joblib, numpy as np, xgboost as xgb, torch\n",
        "from typing import Dict\n",
        "\n",
        "def load_all(art_dir: str) -> Dict:\n",
        "    booster = xgb.Booster(); booster.load_model(f\"{{art_dir}}/xgb_booster.json\")\n",
        "    with open(f\"{{art_dir}}/features.json\") as f: meta = json.load(f)\n",
        "    try:\n",
        "        lstm = torch.jit.load(f\"{{art_dir}}/lstm_torchscript.pt\"); lstm.eval()\n",
        "    except:\n",
        "        lstm = None\n",
        "    try:\n",
        "        sc_x = joblib.load(f\"{{art_dir}}/scaler_X.pkl\")\n",
        "        sc_y = joblib.load(f\"{{art_dir}}/scaler_y.pkl\")\n",
        "    except:\n",
        "        sc_x = sc_y = None\n",
        "    try:\n",
        "        with open(f\"{{art_dir}}/ensemble.json\") as f: ens = json.load(f)\n",
        "        alpha = float(ens.get(\"alpha_lstm\", 0.5))\n",
        "    except:\n",
        "        alpha = 0.5\n",
        "    return dict(booster=booster, meta=meta, lstm=lstm, sc_x=sc_x, sc_y=sc_y, alpha=alpha)\n",
        "\n",
        "def predict_xgb(booster, X_df):\n",
        "    d = xgb.DMatrix(X_df.values)\n",
        "    return booster.predict(d)\n",
        "\n",
        "def predict_lstm(lstm, sc_x, sc_y, X_df, win=30):\n",
        "    if lstm is None or sc_x is None or sc_y is None:\n",
        "        return None\n",
        "    Xs = sc_x.transform(X_df.values).astype(np.float32)\n",
        "    if len(Xs) < win: return None\n",
        "    seq = np.stack([Xs[-win:]], axis=0)  # 1×WIN×F\n",
        "    with torch.no_grad():\n",
        "        yp_s = lstm(torch.from_numpy(seq))\n",
        "    return sc_y.inverse_transform(yp_s.numpy()).ravel()\n",
        "\n",
        "def predict_ensemble(models, X_df):\n",
        "    xgb_pred = predict_xgb(models['booster'], X_df)\n",
        "    lstm_pred = predict_lstm(models['lstm'], models['sc_x'], models['sc_y'], X_df, win=30)\n",
        "    if lstm_pred is None:\n",
        "        return xgb_pred\n",
        "    # letzte XGB/Vergleichslänge\n",
        "    y_l = min(len(xgb_pred), len(lstm_pred))\n",
        "    ens = models['alpha']*lstm_pred[-y_l:] + (1-models['alpha'])*xgb_pred[-y_l:]\n",
        "    return ens\n",
        "\"\"\"\n",
        "with open(os.path.join(ART_DIR, \"predict.py\"), \"w\") as f:\n",
        "    f.write(predictor_py)\n",
        "\n",
        "print(\"✅ Artefakte für Streamlit exportiert nach:\", ART_DIR)"
      ],
      "metadata": {
        "id": "aITooNxO5490"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 19 — MLflow-Logging als Fallback-Wrapper"
      ],
      "metadata": {
        "id": "GT6CfQ8N58zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MLFLOW = True\n",
        "try:\n",
        "    import mlflow, mlflow.sklearn\n",
        "except Exception as e:\n",
        "    print(\"MLflow nicht verfügbar:\", e); USE_MLFLOW = False\n",
        "\n",
        "if USE_MLFLOW:\n",
        "    mlflow.set_experiment(\"Retail Forecast (clean)\")\n",
        "    with mlflow.start_run(run_name=\"xgb_final_backtest\"):\n",
        "        mlflow.set_tag(\"phase\",\"week2_3\")\n",
        "        if 'best_params' in globals(): mlflow.log_params(best_params)\n",
        "        avg_metrics = {k: float(np.nanmean([m[k] for m in fold_metrics])) for k in fold_metrics[0].keys()}\n",
        "        mlflow.log_metrics({f\"avg_{k}\": v for k,v in avg_metrics.items()})\n",
        "        mlflow.log_artifact(os.path.join(ART_DIR, \"xgb_booster.json\"))\n",
        "        mlflow.log_artifact(os.path.join(ART_DIR, \"features.json\"))\n",
        "        if os.path.exists(os.path.join(ART_DIR,\"ensemble.json\")):\n",
        "            mlflow.log_artifact(os.path.join(ART_DIR,\"ensemble.json\"))\n",
        "    print(\"📝 MLflow: Logged.\")"
      ],
      "metadata": {
        "id": "j4AGEuH859vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 20 — Inferenz-Bundle & einheitliche predict_batch()"
      ],
      "metadata": {
        "id": "Bb8IZxkY7aYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "ART_DIR = ART_DIR if 'ART_DIR' in globals() else \"./artifacts_week2_3\"\n",
        "Path(ART_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Booster & Featureliste einsammeln (aus früheren Blöcken)\n",
        "#    Fallback: nimm booster_final, sonst lade aus Datei\n",
        "booster_path = os.path.join(ART_DIR, \"xgb_booster.json\")\n",
        "features_path = os.path.join(ART_DIR, \"features.json\")\n",
        "\n",
        "if 'booster_final' in globals():\n",
        "    booster_final.save_model(booster_path)\n",
        "\n",
        "if 'FEATURES' in globals():\n",
        "    with open(features_path, \"w\") as f:\n",
        "        json.dump({\"features\": FEATURES, \"target\": TARGET}, f, indent=2)\n",
        "\n",
        "# 2) kleines Inferenz-Metapaket\n",
        "bundle = {\n",
        "    \"booster_path\": booster_path,\n",
        "    \"features_path\": features_path,\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"model_type\": \"xgboost_booster\",\n",
        "    \"version\": \"1.0.0\"\n",
        "}\n",
        "with open(os.path.join(ART_DIR, \"inference_bundle.json\"), \"w\") as f:\n",
        "    json.dump(bundle, f, indent=2)\n",
        "\n",
        "print(\"📦 Inferenz-Bundle geschrieben ->\", ART_DIR)\n",
        "\n",
        "# 3) Einfache Vorhersagefunktion für Streamlit/FastAPI\n",
        "def load_bundle(art_dir=ART_DIR):\n",
        "    with open(os.path.join(art_dir, \"features.json\"), \"r\") as f:\n",
        "        cfg = json.load(f)\n",
        "    feats = cfg[\"features\"]\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(os.path.join(art_dir, \"xgb_booster.json\"))\n",
        "    return booster, feats, cfg.get(\"target\")\n",
        "\n",
        "def predict_batch(df_in: pd.DataFrame, art_dir=ART_DIR):\n",
        "    \"\"\"df_in muss die Spalten aus FEATURES bereits enthalten (FE-Pipeline vorher!).\"\"\"\n",
        "    booster, feats, _ = load_bundle(art_dir)\n",
        "    X = df_in[feats].astype(float)\n",
        "    dm = xgb.DMatrix(X)\n",
        "    yhat = booster.predict(dm)\n",
        "    return pd.Series(yhat, index=df_in.index, name=\"pred_xgb\")\n",
        "\n",
        "# Smoke-Test\n",
        "try:\n",
        "    _sm = predict_batch(df.iloc[:5].copy())\n",
        "    print(\"✅ predict_batch() Smoke-Test ok:\", _sm.head().to_list())\n",
        "except Exception as e:\n",
        "    print(\"⚠️ predict_batch() Test fehlgeschlagen:\", e)"
      ],
      "metadata": {
        "id": "oclXO2WS7bI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 21 — Conformal Prediction (robuste Prognoseintervalle)"
      ],
      "metadata": {
        "id": "gn9AluCQ7dab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voraussetzung: bt_df mit Spalten [TARGET, 'pred_bt', 'resid'] aus Block 15/12\n",
        "assert 'bt_df' in globals() and 'resid' in bt_df.columns, \"Backtest-Residuen fehlen (siehe Block 12/15).\"\n",
        "\n",
        "def conformal_interval_from_residuals(residuals: pd.Series, alpha=0.1):\n",
        "    \"\"\"Gibt die additiven Quantil-Schutzhüllen zurück (z.B. 90%-Intervall bei alpha=0.1).\"\"\"\n",
        "    res = residuals.dropna().values\n",
        "    q_low  = np.quantile(res, alpha/2)\n",
        "    q_high = np.quantile(res, 1 - alpha/2)\n",
        "    return float(q_low), float(q_high)\n",
        "\n",
        "# Intervall aufbauen (90% und 95%)\n",
        "qL90, qH90 = conformal_interval_from_residuals(bt_df[\"resid\"], alpha=0.10)\n",
        "qL95, qH95 = conformal_interval_from_residuals(bt_df[\"resid\"], alpha=0.05)\n",
        "print(f\"📏 Conformal Hüllen: 90% [{qL90:+.3f}, {qH90:+.3f}] | 95% [{qL95:+.3f}, {qH95:+.3f}]\")\n",
        "\n",
        "def apply_conformal(y_pred: pd.Series, qL: float, qH: float):\n",
        "    lo = y_pred + qL\n",
        "    hi = y_pred + qH\n",
        "    return lo.rename(\"pred_lo\"), hi.rename(\"pred_hi\")\n",
        "\n",
        "# Beispiel: aktuelle Test-Vorhersage plus Intervall\n",
        "try:\n",
        "    y_pred_now = predict_batch(df.copy())\n",
        "    lo90, hi90 = apply_conformal(y_pred_now, qL90, qH90)\n",
        "    _check = pd.concat([y_pred_now.head(3), lo90.head(3), hi90.head(3)], axis=1)\n",
        "    print(\"✅ Intervall-Sanity:\\n\", _check)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Intervall-Anwendung fehlgeschlagen:\", e)\n",
        "\n",
        "# Export für Streamlit\n",
        "with open(os.path.join(ART_DIR, \"conformal_intervals.json\"), \"w\") as f:\n",
        "    json.dump({\"qL90\": qL90, \"qH90\": qH90, \"qL95\": qL95, \"qH95\": qH95}, f, indent=2)\n",
        "print(\"💾 conformal_intervals.json gespeichert.\")"
      ],
      "metadata": {
        "id": "tNOBZ0MO7fmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 22 — Mini-Ensemble-Calibrator (Stacking light)"
      ],
      "metadata": {
        "id": "le9TKtSW7mM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "assert 'bt_df' in globals(), \"bt_df fehlt (kommt aus Backtest-Blöcken).\"\n",
        "assert TARGET in bt_df.columns, f\"{TARGET} fehlt in bt_df.\"\n",
        "\n",
        "# Welche Stacking-Quellen sind vorhanden?\n",
        "stack_cols = []\n",
        "if \"pred_bt\" in bt_df.columns:       # XGB-Backtest-Pred\n",
        "    stack_cols.append(\"pred_bt\")\n",
        "if \"pred_lstm_bt\" in bt_df.columns:  # optional: LSTM-Backtest-Pred\n",
        "    stack_cols.append(\"pred_lstm_bt\")\n",
        "\n",
        "if len(stack_cols) == 0:\n",
        "    apply_calibrator = None\n",
        "    print(\"ℹ️ Kein Stacking möglich (keine Stacking-Quellen in bt_df).\")\n",
        "else:\n",
        "    # y & X aufbauen\n",
        "    y_bt_full = bt_df[TARGET].to_numpy(dtype=float)\n",
        "    X_full = np.column_stack([bt_df[c].to_numpy(dtype=float) for c in stack_cols])\n",
        "\n",
        "    # gültige Zeilen (kein NaN/inf in X oder y)\n",
        "    mask = np.isfinite(y_bt_full)\n",
        "    mask &= np.all(np.isfinite(X_full), axis=1)\n",
        "\n",
        "    n_all = len(y_bt_full)\n",
        "    n_keep = int(mask.sum())\n",
        "    n_drop = n_all - n_keep\n",
        "    if n_keep < 20:\n",
        "        apply_calibrator = None\n",
        "        print(f\"⚠️ Zu wenig saubere Daten für Calibrator (keep={n_keep}, drop={n_drop}). Überspringe Stacking.\")\n",
        "    else:\n",
        "        y_bt = y_bt_full[mask]\n",
        "        X_stack = X_full[mask]\n",
        "\n",
        "        # Mediane für spätere Imputation merken\n",
        "        col_medians = np.nanmedian(X_stack, axis=0)\n",
        "\n",
        "        # Ridge-Fit\n",
        "        cal = Ridge(alpha=1.0, fit_intercept=True)\n",
        "        cal.fit(X_stack, y_bt)\n",
        "\n",
        "        # Logging\n",
        "        print(f\"🧪 Calibrator fit: genutzt={n_keep}, entfernt={n_drop}\")\n",
        "        weights = dict(zip(stack_cols, cal.coef_.tolist()))\n",
        "        print(\"   Intercept:\", float(cal.intercept_))\n",
        "        print(\"   Gewichte:\", {k: round(v, 5) for k, v in weights.items()})\n",
        "\n",
        "        # Anwendungsfunktion\n",
        "        def apply_calibrator(df_feats: pd.DataFrame) -> pd.Series:\n",
        "            \"\"\"\n",
        "            Erwartet:\n",
        "              - XGB-Pred über predict_batch(df_feats) (aus Block 20)\n",
        "              - optional df_feats['pred_lstm'] falls du LSTM schon extern berechnet hast\n",
        "            Gibt kalibrierte Vorhersage 'pred_cal' zurück.\n",
        "            \"\"\"\n",
        "            preds_list = []\n",
        "            # XGB immer\n",
        "            if 'predict_batch' not in globals():\n",
        "                raise RuntimeError(\"predict_batch() nicht gefunden (siehe Block 20).\")\n",
        "            yx = predict_batch(df_feats).to_numpy(dtype=float)\n",
        "            preds_list.append(yx)\n",
        "\n",
        "            # ggf. LSTM (falls als Spalte vorhanden)\n",
        "            if \"pred_lstm\" in df_feats.columns and \"pred_lstm_bt\" in stack_cols:\n",
        "                preds_list.append(df_feats[\"pred_lstm\"].to_numpy(dtype=float))\n",
        "            elif \"pred_lstm_bt\" in stack_cols:\n",
        "                # wenn im Calibrator LSTM erwartet wird, aber hier fehlt: mit NaN auffüllen\n",
        "                preds_list.append(np.full_like(yx, np.nan, dtype=float))\n",
        "\n",
        "            Xnew = np.column_stack(preds_list)\n",
        "\n",
        "            # NaNs impute mit Trainingsmedianen je Spalte\n",
        "            # (Anzahl Spalten von Xnew kann <len(stack_cols) sein, daher mappen)\n",
        "            # Reihenfolge der Spalten in Xnew entspricht:\n",
        "            #   ['pred_bt'] (+ optional 'pred_lstm') – passend zu stack_cols\n",
        "            if Xnew.shape[1] != len(stack_cols):\n",
        "                # Mappen: baue die Matrix exakt in stack_cols-Reihenfolge\n",
        "                mats = []\n",
        "                pos = 0\n",
        "                for col in stack_cols:\n",
        "                    if col == \"pred_bt\":\n",
        "                        mats.append(yx)\n",
        "                    elif col == \"pred_lstm_bt\":\n",
        "                        v = df_feats[\"pred_lstm\"].to_numpy(dtype=float) if \"pred_lstm\" in df_feats.columns else np.full_like(yx, np.nan)\n",
        "                        mats.append(v)\n",
        "                Xnew = np.column_stack(mats)\n",
        "\n",
        "            # Imputation\n",
        "            for j in range(Xnew.shape[1]):\n",
        "                col = Xnew[:, j]\n",
        "                miss = ~np.isfinite(col)\n",
        "                if np.any(miss):\n",
        "                    Xnew[miss, j] = col_medians[j]\n",
        "\n",
        "            y_cal = cal.predict(Xnew)\n",
        "            return pd.Series(y_cal, index=df_feats.index, name=\"pred_cal\")\n",
        "\n",
        "        print(\"✅ apply_calibrator() bereit.\")"
      ],
      "metadata": {
        "id": "rZk5sKVJ7m-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 23 — Drift-Check (PSI) zwischen Train und “jüngster” Periode"
      ],
      "metadata": {
        "id": "SB-CClxg85Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def psi_score(a: np.ndarray, b: np.ndarray, bins=10):\n",
        "    a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
        "    if len(a) < 10 or len(b) < 10:\n",
        "        return np.nan\n",
        "    qs = np.quantile(a, np.linspace(0, 1, bins+1))\n",
        "    qs[0], qs[-1] = -np.inf, np.inf\n",
        "    a_bin = np.histogram(a, bins=qs)[0] / max(1, len(a))\n",
        "    b_bin = np.histogram(b, bins=qs)[0] / max(1, len(b))\n",
        "    # Glättung gegen log(0)\n",
        "    a_bin = np.clip(a_bin, 1e-6, 1); b_bin = np.clip(b_bin, 1e-6, 1)\n",
        "    return float(np.sum((b_bin - a_bin) * np.log(b_bin / a_bin)))\n",
        "\n",
        "# Referenz = Trainingsfenster (train_all), Vergleich = letztes Testfenster (test)\n",
        "ref_df = train_all.copy()\n",
        "cmp_df = test.copy() if 'test' in globals() else df.tail(2000).copy()\n",
        "\n",
        "psi_tbl = []\n",
        "for c in FEATURES:\n",
        "    try:\n",
        "        s = psi_score(ref_df[c].astype(float).values, cmp_df[c].astype(float).values, bins=10)\n",
        "        psi_tbl.append((c, s))\n",
        "    except Exception:\n",
        "        psi_tbl.append((c, np.nan))\n",
        "\n",
        "psi_df = pd.DataFrame(psi_tbl, columns=[\"feature\",\"psi\"]).sort_values(\"psi\", ascending=False)\n",
        "print(\"📊 Top-Drifts (PSI>0.2 ~ moderat, >0.3 stark):\")\n",
        "try:\n",
        "    display(psi_df.head(15))\n",
        "except NameError:\n",
        "    print(psi_df.head(15))\n",
        "psi_df.to_csv(os.path.join(ART_DIR, \"psi_latest.csv\"), index=False)\n",
        "print(\"💾 psi_latest.csv gespeichert.\")"
      ],
      "metadata": {
        "id": "VcmdlLyP86A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧱 Block 24 — Handover für Streamlit (kleine Schablone + Artefakt-Load)"
      ],
      "metadata": {
        "id": "GV0XzolP89Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stub_art_dir = ART_DIR\n",
        "\n",
        "streamlit_stub = f'''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import json, os\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "ART_DIR = \"{stub_art_dir}\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_artifacts():\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(os.path.join(ART_DIR, \"xgb_booster.json\"))\n",
        "    with open(os.path.join(ART_DIR, \"features.json\")) as f:\n",
        "        feats = json.load(f)[\"features\"]\n",
        "    # Conformal optional\n",
        "    conf = None\n",
        "    p = os.path.join(ART_DIR, \"conformal_intervals.json\")\n",
        "    if os.path.exists(p):\n",
        "        with open(p) as f:\n",
        "            conf = json.load(f)\n",
        "    return booster, feats, conf\n",
        "\n",
        "def ensure_feature_frame(df, feats):\n",
        "    # Nur gemeinsame Spalten in korrekter Reihenfolge; fehlende warnen\n",
        "    missing = [c for c in feats if c not in df.columns]\n",
        "    if missing:\n",
        "        st.warning(f\"Fehlende Spalten in Upload: {{missing}}\")\n",
        "    use_cols = [c for c in feats if c in df.columns]\n",
        "    if len(use_cols) == 0:\n",
        "        raise ValueError(\"Keine der erwarteten Feature-Spalten vorhanden.\")\n",
        "    return df[use_cols].astype(float), use_cols\n",
        "\n",
        "def predict(booster, feats, df):\n",
        "    X, used = ensure_feature_frame(df, feats)\n",
        "    dm = xgb.DMatrix(X)\n",
        "    yhat = booster.predict(dm)\n",
        "    return yhat\n",
        "\n",
        "st.title(\"🛍️ Retail Forecast – XGB\")\n",
        "booster, FEATURES, CONF = load_artifacts()\n",
        "\n",
        "st.sidebar.header(\"CSV Upload\")\n",
        "file = st.sidebar.file_uploader(\"CSV mit bereits vorbereiteten Features\", type=[\"csv\"])\n",
        "if file is not None:\n",
        "    df = pd.read_csv(file)\n",
        "    st.write(\"Vorschau:\", df.head())\n",
        "\n",
        "    try:\n",
        "        yhat = predict(booster, FEATURES, df)\n",
        "        out = pd.DataFrame({{\"pred_xgb\": yhat}})\n",
        "        if CONF:\n",
        "            qL, qH = CONF.get(\"qL90\"), CONF.get(\"qH90\")\n",
        "            if qL is not None and qH is not None:\n",
        "                out[\"pred_lo90\"] = yhat + float(qL)\n",
        "                out[\"pred_hi90\"] = yhat + float(qH)\n",
        "\n",
        "        st.subheader(\"Vorhersagen\")\n",
        "        st.dataframe(out.head(200))\n",
        "        st.download_button(\n",
        "            \"Ergebnisse als CSV\",\n",
        "            out.to_csv(index=False),\n",
        "            \"predictions.csv\",\n",
        "            \"text/csv\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Fehler bei der Vorhersage: {{e}}\")\n",
        "else:\n",
        "    st.info(\"Bitte CSV hochladen (mit denselben Spalten wie im Training).\")\n",
        "'''\n",
        "\n",
        "stub_path = os.path.join(ART_DIR, \"streamlit_app_stub.py\")\n",
        "with open(stub_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(streamlit_stub)\n",
        "\n",
        "print(\"🧩 Streamlit-Starter abgelegt ->\", stub_path)"
      ],
      "metadata": {
        "id": "cK8zori18-kB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}